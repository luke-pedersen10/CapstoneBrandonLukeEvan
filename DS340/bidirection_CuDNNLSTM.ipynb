{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Bidirectional, Dropout, Conv1D, MaxPooling1D, Flatten, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(ticker, start='2024-01-01', end='2025-02-25'):\n",
    "    stock = yf.download(ticker, start=start, end=end)['Close']\n",
    "    return stock.pct_change().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle function\n",
    "\n",
    "def make_model():\n",
    "    inp = Input(shape=(128, 10))\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(inp)\n",
    "    x = Bidirectional(LSTM(32, return_sequences=True))(x)\n",
    "    x = Attention(128)(x)\n",
    "    # A intermediate full connected (Dense) can help to deal with nonlinears outputs\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dense(9, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My function\n",
    "def make_model():\n",
    "    # 1. Input layer for stock data (sequence_length, features)\n",
    "    inp = Input(shape=(128, 10))  # Example: 128 timesteps, 10 features (adjust as needed)\n",
    "\n",
    "    # 2. Bidirectional LSTM layers for sequence learning\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(inp)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "\n",
    "    # 3. Dropout to prevent overfitting\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # 4. 1D Convolutional layer for feature extraction\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "    # 5. MaxPooling to downsample\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # 6. Flatten layer to transition to fully connected layers\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # 7. Fully connected layers for high-level learning\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "\n",
    "    # 8. Output layer - Single neuron with tanh activation to produce values between -1 and 1\n",
    "    output = Dense(1, activation=\"tanh\")(x)\n",
    "\n",
    "    # 9. Compile model with Adam optimizer\n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = make_model()\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
